apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: nyc-taxi-vendor-count
  namespace: spark
spec:
  type: Python
  mode: cluster
  image: ghcr.io/tripl-ai/arc:arc_3.11.1_spark_3.1.2_scala_2.12_hadoop_3.2.0_1.0.0
  mainApplicationFile: "s3a://$(BUCKET_PARAM)/app_code/job/NYCTaxiCount.py"
  arguments: ["s3a://nyc-tlc/csv_backup/green_tripdata*.csv"]
  sparkVersion: "3.1.2"
  sparkConf:
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "com.amazonaws.auth.WebIdentityTokenCredentialsProvider"
    "spark.kubernetes.allocation.batch.size": "10"
  dynamicAllocation:
    enabled: true
    maxExecutors: 20
  driver:
    env:
      - name: BUCKET_PARAM
        valueFrom:
          configMapKeyRef:
            name: special-config
            key: appcodeBucket
    cores: 1
    memory: "1G"
    labels:
      role: driver
    serviceAccount: nativejob
  executor:
    cores: 1
    memory: "6G"
    labels:
      role: executor